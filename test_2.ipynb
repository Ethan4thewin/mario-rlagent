{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import transforms as T\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from collections import deque\n",
    "import random, datetime, os\n",
    "import time, datetime\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Gym is an OpenAI toolkit for RL\n",
    "import gym\n",
    "from gym.spaces import Box\n",
    "from gym.wrappers import FrameStack\n",
    "\n",
    "# NES Emulator for OpenAI Gym\n",
    "from nes_py.wrappers import JoypadSpace\n",
    "\n",
    "# Super Mario environment for OpenAI Gym\n",
    "import gym_super_mario_bros\n",
    "\n",
    "from tensordict import TensorDict\n",
    "from torchrl.data import TensorDictReplayBuffer, LazyMemmapStorage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(240, 256, 3),\n",
      " 0.0,\n",
      " False,\n",
      " {'coins': 0, 'flag_get': False, 'life': 2, 'score': 0, 'stage': 1, 'status': 'small', 'time': 400, 'world': 1, 'x_pos': 40, 'y_pos': 79}\n"
     ]
    }
   ],
   "source": [
    "# Initialize Super Mario environment (in v0.26 change render mode to 'human' to see results on the screen)\n",
    "if gym.__version__ < '0.26':\n",
    "    env = gym_super_mario_bros.make(\"SuperMarioBros-1-1-v0\", new_step_api=True)\n",
    "else:\n",
    "    env = gym_super_mario_bros.make(\"SuperMarioBros-1-1-v0\", render_mode='rgb', apply_api_compatibility=True)\n",
    "\n",
    "# Define the movement options for Super Mario\n",
    "MOVEMENT_OPTIONS = [\n",
    "    [\"right\"],        # Move right\n",
    "    [\"right\", \"A\"],   # Jump right\n",
    "]\n",
    "\n",
    "# Apply the wrapper to the environment\n",
    "env = JoypadSpace(env, MOVEMENT_OPTIONS)\n",
    "\n",
    "env.reset()\n",
    "next_state, reward, done, trunc, info = env.step(action=0)\n",
    "print(f\"{next_state.shape},\\n {reward},\\n {done},\\n {info}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the environment\n",
    "class SkipFrame(gym.Wrapper):\n",
    "    def __init__(self, env, skip):\n",
    "        \"\"\"Return only every `skip`-th frame\"\"\"\n",
    "        super().__init__(env)\n",
    "        self._skip = skip\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"Repeat action, and sum reward\"\"\"\n",
    "        total_reward = 0.0\n",
    "        for i in range(self._skip):\n",
    "            # Accumulate reward and repeat the same action\n",
    "            obs, reward, done, trunk, info = self.env.step(action)\n",
    "            total_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "        return obs, total_reward, done, trunk, info\n",
    "\n",
    "\n",
    "class GrayScaleObservation(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        obs_shape = self.observation_space.shape[:2]\n",
    "        self.observation_space = Box(low=0, high=255, shape=obs_shape, dtype=np.uint8)\n",
    "\n",
    "    def permute_orientation(self, observation):\n",
    "        # permute [H, W, C] array to [C, H, W] tensor\n",
    "        observation = np.transpose(observation, (2, 0, 1))\n",
    "        observation = torch.tensor(observation.copy(), dtype=torch.float)\n",
    "        return observation\n",
    "\n",
    "    def observation(self, observation):\n",
    "        observation = self.permute_orientation(observation)\n",
    "        transform = T.Grayscale()\n",
    "        observation = transform(observation)\n",
    "        return observation\n",
    "\n",
    "\n",
    "class ResizeObservation(gym.ObservationWrapper):\n",
    "    def __init__(self, env, shape):\n",
    "        super().__init__(env)\n",
    "        if isinstance(shape, int):\n",
    "            self.shape = (shape, shape)\n",
    "        else:\n",
    "            self.shape = tuple(shape)\n",
    "\n",
    "        obs_shape = self.shape + self.observation_space.shape[2:]\n",
    "        self.observation_space = Box(low=0, high=255, shape=obs_shape, dtype=np.uint8)\n",
    "\n",
    "    def observation(self, observation):\n",
    "        transforms = T.Compose(\n",
    "            [T.Resize(self.shape, antialias=True), T.Normalize(0, 255)]\n",
    "        )\n",
    "        observation = transforms(observation).squeeze(0)\n",
    "        return observation\n",
    "\n",
    "\n",
    "# Apply Wrappers to environment\n",
    "env = SkipFrame(env, skip=4)\n",
    "env = GrayScaleObservation(env)\n",
    "env = ResizeObservation(env, shape=84)\n",
    "if gym.__version__ < '0.26':\n",
    "    env = FrameStack(env, num_stack=4, new_step_api=True)\n",
    "else:\n",
    "    env = FrameStack(env, num_stack=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mario:\n",
    "    def __init__(self, state_dim, action_dim, save_dir):\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.save_dir = save_dir\n",
    "\n",
    "        self.device = \"cuda\"\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "            print(f\"Memory Allocated: {torch.cuda.memory_allocated(0) / 1024**2:.2f} MB\")\n",
    "        else:\n",
    "            print(\"CUDA is not available. Using CPU.\")\n",
    "\n",
    "        # Mario's DNN to predict the most optimal action - we implement this in the Learn section\n",
    "        self.net = MarioNet(self.state_dim, self.action_dim).float().to(device=self.device)\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            print(f\"Memory Allocated after model creation: {torch.cuda.memory_allocated(0) / 1024**2:.2f} MB\")\n",
    "\n",
    "        self.exploration_rate = 1\n",
    "        self.exploration_rate_decay = 0.99999 # Slower decay for more exploration\n",
    "        self.exploration_rate_min = 0.15 # Higher minimum for more exploration\n",
    "        self.curr_step = 0\n",
    "\n",
    "        self.save_every = 5e5  # no. of experiences between saving Mario Net\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            print(f\"Initial GPU Memory Allocated: {torch.cuda.memory_allocated(0) / 1024**2:.2f} MB\")\n",
    "\n",
    "    def act(self, state):\n",
    "        \"\"\"\n",
    "    Given a state, choose an epsilon-greedy action and update value of step.\n",
    "\n",
    "    Inputs:\n",
    "    state(``LazyFrame``): A single observation of the current state, dimension is (state_dim)\n",
    "    Outputs:\n",
    "    ``action_idx`` (``int``): An integer representing which action Mario will perform\n",
    "    \"\"\"\n",
    "        # EXPLORE\n",
    "        if np.random.rand() < self.exploration_rate:\n",
    "            action_idx = np.random.randint(self.action_dim)\n",
    "\n",
    "        # EXPLOIT\n",
    "        else:\n",
    "            state = state[0].__array__() if isinstance(state, tuple) else state.__array__()\n",
    "            state = torch.tensor(state, device=self.device).unsqueeze(0)\n",
    "            action_values = self.net(state, model=\"online\")\n",
    "            action_idx = torch.argmax(action_values, axis=1).item()\n",
    "\n",
    "        # decrease exploration_rate\n",
    "        self.exploration_rate *= self.exploration_rate_decay\n",
    "        self.exploration_rate = max(self.exploration_rate_min, self.exploration_rate)\n",
    "\n",
    "        # increment step\n",
    "        self.curr_step += 1\n",
    "        return action_idx\n",
    "\n",
    "class Mario(Mario):  # subclassing for continuity\n",
    "    def __init__(self, state_dim, action_dim, save_dir):\n",
    "        super().__init__(state_dim, action_dim, save_dir)\n",
    "        self.memory = TensorDictReplayBuffer(storage=LazyMemmapStorage(100000, device=torch.device(\"cpu\")))\n",
    "        self.batch_size = 256 # larger batch size for more stable learning\n",
    "\n",
    "    def cache(self, state, next_state, action, reward, done):\n",
    "        \"\"\"\n",
    "        Store the experience to self.memory (replay buffer)\n",
    "\n",
    "        Inputs:\n",
    "        state (``LazyFrame``),\n",
    "        next_state (``LazyFrame``),\n",
    "        action (``int``),\n",
    "        reward (``float``),\n",
    "        done(``bool``))\n",
    "        \"\"\"\n",
    "        def first_if_tuple(x):\n",
    "            return x[0] if isinstance(x, tuple) else x\n",
    "        state = first_if_tuple(state).__array__()\n",
    "        next_state = first_if_tuple(next_state).__array__()\n",
    "\n",
    "        state = torch.tensor(state)\n",
    "        next_state = torch.tensor(next_state)\n",
    "        action = torch.tensor([action])\n",
    "        reward = torch.tensor([reward])\n",
    "        done = torch.tensor([done])\n",
    "\n",
    "        # self.memory.push(state, next_state, action, reward, done)\n",
    "        self.memory.add(TensorDict({\"state\": state, \"next_state\": next_state, \"action\": action, \"reward\": reward, \"done\": done}, batch_size=[]))\n",
    "\n",
    "    def recall(self):\n",
    "        \"\"\"\n",
    "        Retrieve a batch of experiences from memory\n",
    "        \"\"\"\n",
    "        return self.memory.sample(self.batch_size)\n",
    "\n",
    "class MarioNet(nn.Module):\n",
    "    \"\"\"mini CNN structure\n",
    "  input -> (conv2d + relu) x 3 -> flatten -> (dense + relu) x 2 -> output\n",
    "  \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        c, h, w = input_dim\n",
    "\n",
    "        if h != 84:\n",
    "            raise ValueError(f\"Expecting input height: 84, got: {h}\")\n",
    "        if w != 84:\n",
    "            raise ValueError(f\"Expecting input width: 84, got: {w}\")\n",
    "\n",
    "        self.online = self.__build_cnn(c, output_dim)\n",
    "\n",
    "        self.target = self.__build_cnn(c, output_dim)\n",
    "        self.target.load_state_dict(self.online.state_dict())\n",
    "\n",
    "        # Q_target parameters are frozen.\n",
    "        for p in self.target.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "    def forward(self, input, model):\n",
    "        if model == \"online\":\n",
    "            return self.online(input)\n",
    "        elif model == \"target\":\n",
    "            return self.target(input)\n",
    "\n",
    "    def __build_cnn(self, c, output_dim):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels=c, out_channels=32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(3136, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, output_dim),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ethan\\AppData\\Local\\Temp\\ipykernel_17720\\1023324345.py:6: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  @torch.cuda.amp.autocast()\n",
      "C:\\Users\\Ethan\\AppData\\Local\\Temp\\ipykernel_17720\\1023324345.py:13: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  @torch.cuda.amp.autocast()\n"
     ]
    }
   ],
   "source": [
    "class Mario(Mario):\n",
    "    def __init__(self, state_dim, action_dim, save_dir):\n",
    "        super().__init__(state_dim, action_dim, save_dir)\n",
    "        self.gamma = 0.98 # Higher discount factor for long-term rewards\n",
    "\n",
    "    @torch.cuda.amp.autocast()\n",
    "    def td_estimate(self, state, action):\n",
    "        current_Q = self.net(state, model=\"online\")[\n",
    "            np.arange(0, self.batch_size), action\n",
    "        ]  # Q_online(s,a)\n",
    "        return current_Q\n",
    "\n",
    "    @torch.cuda.amp.autocast()\n",
    "    @torch.no_grad()\n",
    "    def td_target(self, reward, next_state, done):\n",
    "        next_state_Q = self.net(next_state, model=\"online\")\n",
    "        best_action = torch.argmax(next_state_Q, axis=1)\n",
    "        next_Q = self.net(next_state, model=\"target\")[\n",
    "            np.arange(0, self.batch_size), best_action\n",
    "        ]\n",
    "        return (reward + (1 - done.float()) * self.gamma * next_Q).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mario(Mario):\n",
    "    def __init__(self, state_dim, action_dim, save_dir):\n",
    "        super().__init__(state_dim, action_dim, save_dir)\n",
    "        self.optimizer = torch.optim.Adam(self.net.parameters(), lr=0.0005)\n",
    "        self.loss_fn = torch.nn.SmoothL1Loss()\n",
    "\n",
    "    def update_Q_online(self, td_estimate, td_target):\n",
    "        loss = self.loss_fn(td_estimate, td_target)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        return loss.item()\n",
    "\n",
    "    def sync_Q_target(self):\n",
    "        self.net.target.load_state_dict(self.net.online.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mario(Mario):\n",
    "    def save(self):\n",
    "        save_path = (\n",
    "            self.save_dir / f\"mario_net_{int(self.curr_step // self.save_every)}.chkpt\"\n",
    "        )\n",
    "        torch.save(\n",
    "            dict(model=self.net.state_dict(), exploration_rate=self.exploration_rate),\n",
    "            save_path,\n",
    "        )\n",
    "        print(f\"MarioNet saved to {save_path} at step {self.curr_step}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mario(Mario):\n",
    "    def __init__(self, state_dim, action_dim, save_dir):\n",
    "        super().__init__(state_dim, action_dim, save_dir)\n",
    "        self.burnin = 5000  # min. experiences before training\n",
    "        self.learn_every = 2  # no. of experiences between updates to Q_online\n",
    "        self.sync_every = 5000  # no. of experiences between Q_target & Q_online sync\n",
    "\n",
    "    def learn(self):\n",
    "        if self.curr_step % self.sync_every == 0:\n",
    "            self.sync_Q_target()\n",
    "\n",
    "        if self.curr_step % self.save_every == 0:\n",
    "            self.save()\n",
    "\n",
    "        if self.curr_step < self.burnin:\n",
    "            return None, None\n",
    "\n",
    "        if self.curr_step % self.learn_every != 0:\n",
    "            return None, None\n",
    "\n",
    "        # Sample from memory\n",
    "        state, next_state, action, reward, done = self.recall()\n",
    "\n",
    "        # Get TD Estimate\n",
    "        td_est = self.td_estimate(state, action)\n",
    "\n",
    "        # Get TD Target\n",
    "        td_tgt = self.td_target(reward, next_state, done)\n",
    "\n",
    "        # Backpropagate loss through Q_online\n",
    "        loss = self.update_Q_online(td_est, td_tgt)\n",
    "\n",
    "        # Print training stats periodically\n",
    "        if self.curr_step % 1000 == 0:\n",
    "            print(f\"Step: {self.curr_step} \"\n",
    "                  f\"TD Est Mean: {td_est.mean().item():.4f}, \"\n",
    "                  f\"Exploration Rate: {self.exploration_rate:.4f}\")\n",
    "\n",
    "        if self.curr_step % 1000 == 0 and torch.cuda.is_available():\n",
    "            print(f\"GPU Memory Allocated: {torch.cuda.memory_allocated(0) / 1024**2:.2f} MB\")\n",
    "\n",
    "        return (td_est.mean().item(), loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetricLogger:\n",
    "    def __init__(self, save_dir):\n",
    "        self.save_log = save_dir / \"log\"\n",
    "        with open(self.save_log, \"w\") as f:\n",
    "            f.write(\n",
    "                f\"{'Episode':>8}{'Step':>8}{'Epsilon':>10}{'MeanReward':>15}\"\n",
    "                f\"{'MeanLength':>15}{'MeanLoss':>15}{'MeanQValue':>15}\"\n",
    "                f\"{'TimeDelta':>15}{'Time':>20}\\n\"\n",
    "            )\n",
    "        self.ep_rewards_plot = save_dir / \"reward_plot.jpg\"\n",
    "        self.ep_lengths_plot = save_dir / \"length_plot.jpg\"\n",
    "        self.ep_avg_losses_plot = save_dir / \"loss_plot.jpg\"\n",
    "        self.ep_avg_qs_plot = save_dir / \"q_plot.jpg\"\n",
    "\n",
    "        # History metrics\n",
    "        self.ep_rewards = []\n",
    "        self.ep_lengths = []\n",
    "        self.ep_avg_losses = []\n",
    "        self.ep_avg_qs = []\n",
    "\n",
    "        # Moving averages, added for every call to record()\n",
    "        self.moving_avg_ep_rewards = []\n",
    "        self.moving_avg_ep_lengths = []\n",
    "        self.moving_avg_ep_avg_losses = []\n",
    "        self.moving_avg_ep_avg_qs = []\n",
    "\n",
    "        # Current episode metric\n",
    "        self.init_episode()\n",
    "\n",
    "        # Timing\n",
    "        self.record_time = time.time()\n",
    "\n",
    "    def log_step(self, reward, loss, q):\n",
    "        self.curr_ep_reward += reward\n",
    "        self.curr_ep_length += 1\n",
    "        if loss:\n",
    "            self.curr_ep_loss += loss\n",
    "            self.curr_ep_q += q\n",
    "            self.curr_ep_loss_length += 1\n",
    "\n",
    "    def log_episode(self):\n",
    "        \"Mark end of episode\"\n",
    "        self.ep_rewards.append(self.curr_ep_reward)\n",
    "        self.ep_lengths.append(self.curr_ep_length)\n",
    "        if self.curr_ep_loss_length == 0:\n",
    "            ep_avg_loss = 0\n",
    "            ep_avg_q = 0\n",
    "        else:\n",
    "            ep_avg_loss = np.round(self.curr_ep_loss / self.curr_ep_loss_length, 5)\n",
    "            ep_avg_q = np.round(self.curr_ep_q / self.curr_ep_loss_length, 5)\n",
    "        self.ep_avg_losses.append(ep_avg_loss)\n",
    "        self.ep_avg_qs.append(ep_avg_q)\n",
    "\n",
    "        self.init_episode()\n",
    "\n",
    "    def init_episode(self):\n",
    "        self.curr_ep_reward = 0.0\n",
    "        self.curr_ep_length = 0\n",
    "        self.curr_ep_loss = 0.0\n",
    "        self.curr_ep_q = 0.0\n",
    "        self.curr_ep_loss_length = 0\n",
    "\n",
    "    def record(self, episode, epsilon, step):\n",
    "        mean_ep_reward = np.round(np.mean(self.ep_rewards[-100:]), 3)\n",
    "        mean_ep_length = np.round(np.mean(self.ep_lengths[-100:]), 3)\n",
    "        mean_ep_loss = np.round(np.mean(self.ep_avg_losses[-100:]), 3)\n",
    "        mean_ep_q = np.round(np.mean(self.ep_avg_qs[-100:]), 3)\n",
    "        self.moving_avg_ep_rewards.append(mean_ep_reward)\n",
    "        self.moving_avg_ep_lengths.append(mean_ep_length)\n",
    "        self.moving_avg_ep_avg_losses.append(mean_ep_loss)\n",
    "        self.moving_avg_ep_avg_qs.append(mean_ep_q)\n",
    "\n",
    "        last_record_time = self.record_time\n",
    "        self.record_time = time.time()\n",
    "        time_since_last_record = np.round(self.record_time - last_record_time, 3)\n",
    "\n",
    "        print(\n",
    "            f\"Episode {episode} - \"\n",
    "            f\"Step {step} - \"\n",
    "            f\"Epsilon {epsilon} - \"\n",
    "            f\"Mean Reward {mean_ep_reward} - \"\n",
    "            f\"Mean Length {mean_ep_length} - \"\n",
    "            f\"Mean Loss {mean_ep_loss} - \"\n",
    "            f\"Mean Q Value {mean_ep_q} - \"\n",
    "            f\"Time Delta {time_since_last_record} - \"\n",
    "            f\"Time {datetime.datetime.now().strftime('%Y-%m-%dT%H:%M:%S')}\"\n",
    "        )\n",
    "\n",
    "        with open(self.save_log, \"a\") as f:\n",
    "            f.write(\n",
    "                f\"{episode:8d}{step:8d}{epsilon:10.3f}\"\n",
    "                f\"{mean_ep_reward:15.3f}{mean_ep_length:15.3f}{mean_ep_loss:15.3f}{mean_ep_q:15.3f}\"\n",
    "                f\"{time_since_last_record:15.3f}\"\n",
    "                f\"{datetime.datetime.now().strftime('%Y-%m-%dT%H:%M:%S'):>20}\\n\"\n",
    "            )\n",
    "\n",
    "        for metric in [\"ep_lengths\", \"ep_avg_losses\", \"ep_avg_qs\", \"ep_rewards\"]:\n",
    "            plt.clf()\n",
    "            plt.plot(getattr(self, f\"moving_avg_{metric}\"), label=f\"moving_avg_{metric}\")\n",
    "            plt.legend()\n",
    "            plt.savefig(getattr(self, f\"{metric}_plot\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA: True\n",
      "\n"
     ]
    }
   ],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "print(f\"Using CUDA: {use_cuda}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU: NVIDIA GeForce RTX 3060 Ti\n",
      "Memory Allocated: 12.86 MB\n",
      "Memory Allocated after model creation: 25.72 MB\n",
      "Initial GPU Memory Allocated: 25.72 MB\n",
      "\n",
      "Current GPU utilization:\n",
      "Memory Allocated: 25.72 MB\n",
      "Memory Cached: 42.00 MB\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[WinError 1455] The paging file is too small for this operation to complete",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 32\u001b[0m\n\u001b[0;32m     29\u001b[0m next_state, reward, done, trunc, info \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# Remember\u001b[39;00m\n\u001b[1;32m---> 32\u001b[0m \u001b[43mmario\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcache\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnext_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdone\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# Learn\u001b[39;00m\n\u001b[0;32m     35\u001b[0m q, loss \u001b[38;5;241m=\u001b[39m mario\u001b[38;5;241m.\u001b[39mlearn()\n",
      "Cell \u001b[1;32mIn[15], line 88\u001b[0m, in \u001b[0;36mMario.cache\u001b[1;34m(self, state, next_state, action, reward, done)\u001b[0m\n\u001b[0;32m     85\u001b[0m done \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([done])\n\u001b[0;32m     87\u001b[0m \u001b[38;5;66;03m# self.memory.push(state, next_state, action, reward, done)\u001b[39;00m\n\u001b[1;32m---> 88\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmemory\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mTensorDict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstate\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnext_state\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mnext_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43maction\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mreward\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdone\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mdone\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Ethan\\miniconda3\\envs\\torch-rl\\lib\\site-packages\\torchrl\\data\\replay_buffers\\replay_buffers.py:1121\u001b[0m, in \u001b[0;36mTensorDictReplayBuffer.add\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m   1118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1119\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_storage\u001b[38;5;241m.\u001b[39mndim), dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong)\n\u001b[1;32m-> 1121\u001b[0m index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_add\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1122\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1123\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_tensor_collection(data):\n",
      "File \u001b[1;32mc:\\Users\\Ethan\\miniconda3\\envs\\torch-rl\\lib\\site-packages\\torchrl\\data\\replay_buffers\\replay_buffers.py:544\u001b[0m, in \u001b[0;36mReplayBuffer._add\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m    542\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_add\u001b[39m(\u001b[38;5;28mself\u001b[39m, data):\n\u001b[0;32m    543\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_replay_lock:\n\u001b[1;32m--> 544\u001b[0m         index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_writer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    545\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler\u001b[38;5;241m.\u001b[39madd(index)\n\u001b[0;32m    546\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m index\n",
      "File \u001b[1;32mc:\\Users\\Ethan\\miniconda3\\envs\\torch-rl\\lib\\site-packages\\torchrl\\data\\replay_buffers\\writers.py:253\u001b[0m, in \u001b[0;36mTensorDictRoundRobinWriter.add\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m    246\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_tensorclass(data):\n\u001b[0;32m    247\u001b[0m     data\u001b[38;5;241m.\u001b[39mset(\n\u001b[0;32m    248\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    249\u001b[0m         expand_as_right(\n\u001b[0;32m    250\u001b[0m             torch\u001b[38;5;241m.\u001b[39mas_tensor(index, device\u001b[38;5;241m=\u001b[39mdata\u001b[38;5;241m.\u001b[39mdevice, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong), data\n\u001b[0;32m    251\u001b[0m         ),\n\u001b[0;32m    252\u001b[0m     )\n\u001b[1;32m--> 253\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_storage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    254\u001b[0m index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_replicate_index(index)\n\u001b[0;32m    255\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ent \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_storage\u001b[38;5;241m.\u001b[39m_attached_entities:\n",
      "File \u001b[1;32mc:\\Users\\Ethan\\miniconda3\\envs\\torch-rl\\lib\\site-packages\\torchrl\\_utils.py:394\u001b[0m, in \u001b[0;36mimplement_for.__call__.<locals>._lazy_call_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    389\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(fn)\n\u001b[0;32m    390\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_lazy_call_fn\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    391\u001b[0m     \u001b[38;5;66;03m# first time we call the function, we also do the replacement.\u001b[39;00m\n\u001b[0;32m    392\u001b[0m     \u001b[38;5;66;03m# This will cause the imports to occur only during the first call to fn\u001b[39;00m\n\u001b[1;32m--> 394\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_delazify\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc_name\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    395\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mc:\\Users\\Ethan\\miniconda3\\envs\\torch-rl\\lib\\site-packages\\torchrl\\data\\replay_buffers\\storages.py:670\u001b[0m, in \u001b[0;36mTensorStorage.set\u001b[1;34m(self, cursor, data, set_cursor)\u001b[0m\n\u001b[0;32m    668\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init(tree_map(\u001b[38;5;28;01mlambda\u001b[39;00m x: x[\u001b[38;5;241m0\u001b[39m], data))\n\u001b[0;32m    669\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 670\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_init\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    671\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_tensor_collection(data):\n\u001b[0;32m    672\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_storage[cursor] \u001b[38;5;241m=\u001b[39m data\n",
      "File \u001b[1;32mc:\\Users\\Ethan\\miniconda3\\envs\\torch-rl\\lib\\site-packages\\torchrl\\data\\replay_buffers\\storages.py:1077\u001b[0m, in \u001b[0;36mLazyMemmapStorage._init\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m   1075\u001b[0m out \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mclone()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m   1076\u001b[0m out \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39mexpand(max_size_along_dim0(data\u001b[38;5;241m.\u001b[39mshape))\n\u001b[1;32m-> 1077\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mout\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmemmap_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscratch_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1078\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, tensor \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msorted\u001b[39m(\n\u001b[0;32m   1079\u001b[0m     out\u001b[38;5;241m.\u001b[39mitems(include_nested\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, leaves_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m), key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m\n\u001b[0;32m   1080\u001b[0m ):\n\u001b[0;32m   1081\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Ethan\\miniconda3\\envs\\torch-rl\\lib\\site-packages\\tensordict\\base.py:4021\u001b[0m, in \u001b[0;36mTensorDictBase.memmap_like\u001b[1;34m(self, prefix, copy_existing, num_threads, return_early, share_non_tensor)\u001b[0m\n\u001b[0;32m   4017\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m TensorDictFuture(futures, result)\n\u001b[0;32m   4018\u001b[0m \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply(\n\u001b[0;32m   4019\u001b[0m     \u001b[38;5;28;01mlambda\u001b[39;00m x: torch\u001b[38;5;241m.\u001b[39mempty((), device\u001b[38;5;241m=\u001b[39mx\u001b[38;5;241m.\u001b[39mdevice, dtype\u001b[38;5;241m=\u001b[39mx\u001b[38;5;241m.\u001b[39mdtype)\u001b[38;5;241m.\u001b[39mexpand(x\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m   4020\u001b[0m )\n\u001b[1;32m-> 4021\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_memmap_\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   4022\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4023\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy_existing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy_existing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4024\u001b[0m \u001b[43m    \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   4025\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlike\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   4026\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexecutor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   4027\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfutures\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   4028\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshare_non_tensor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshare_non_tensor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4029\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mlock_()\n",
      "File \u001b[1;32mc:\\Users\\Ethan\\miniconda3\\envs\\torch-rl\\lib\\site-packages\\tensordict\\_td.py:2488\u001b[0m, in \u001b[0;36mTensorDict._memmap_\u001b[1;34m(self, prefix, copy_existing, executor, futures, inplace, like, share_non_tensor)\u001b[0m\n\u001b[0;32m   2485\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2487\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m executor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 2488\u001b[0m         \u001b[43m_populate_memmap\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2489\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2490\u001b[0m \u001b[43m            \u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2491\u001b[0m \u001b[43m            \u001b[49m\u001b[43mkey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2492\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcopy_existing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy_existing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2493\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2494\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlike\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlike\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2495\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2496\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2497\u001b[0m         futures\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m   2498\u001b[0m             executor\u001b[38;5;241m.\u001b[39msubmit(\n\u001b[0;32m   2499\u001b[0m                 _populate_memmap,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2506\u001b[0m             )\n\u001b[0;32m   2507\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\Ethan\\miniconda3\\envs\\torch-rl\\lib\\site-packages\\tensordict\\_td.py:4329\u001b[0m, in \u001b[0;36m_populate_memmap\u001b[1;34m(dest, value, key, copy_existing, prefix, like)\u001b[0m\n\u001b[0;32m   4327\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   4328\u001b[0m     shape \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 4329\u001b[0m memmap_tensor \u001b[38;5;241m=\u001b[39m \u001b[43mMemoryMappedTensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_tensor\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   4330\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequires_grad\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4331\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4332\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy_existing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy_existing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4333\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexistsok\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   4334\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mlike\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4335\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4336\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4337\u001b[0m dest\u001b[38;5;241m.\u001b[39m_tensordict[key] \u001b[38;5;241m=\u001b[39m memmap_tensor\n\u001b[0;32m   4338\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m memmap_tensor\n",
      "File \u001b[1;32mc:\\Users\\Ethan\\miniconda3\\envs\\torch-rl\\lib\\site-packages\\tensordict\\memmap.py:215\u001b[0m, in \u001b[0;36mMemoryMappedTensor.from_tensor\u001b[1;34m(cls, input, filename, existsok, copy_existing, copy_data, shape)\u001b[0m\n\u001b[0;32m    212\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    213\u001b[0m     \u001b[38;5;66;03m# assume integer\u001b[39;00m\n\u001b[0;32m    214\u001b[0m     size \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39miinfo(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdtype)\u001b[38;5;241m.\u001b[39mbits \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m8\u001b[39m \u001b[38;5;241m*\u001b[39m shape_numel\n\u001b[1;32m--> 215\u001b[0m handler \u001b[38;5;241m=\u001b[39m \u001b[43m_FileHandler\u001b[49m\u001b[43m(\u001b[49m\u001b[43msize\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    216\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(shape, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m    217\u001b[0m     func_offset_stride \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\n\u001b[0;32m    218\u001b[0m         torch, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_nested_compute_contiguous_strides_offsets\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Ethan\\miniconda3\\envs\\torch-rl\\lib\\site-packages\\tensordict\\memmap.py:936\u001b[0m, in \u001b[0;36m_FileHandler.__init__\u001b[1;34m(self, size)\u001b[0m\n\u001b[0;32m    934\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m100\u001b[39m):\n\u001b[0;32m    935\u001b[0m     name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpym-\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m-\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (os\u001b[38;5;241m.\u001b[39mgetpid(), \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_rand))\n\u001b[1;32m--> 936\u001b[0m     buf \u001b[38;5;241m=\u001b[39m \u001b[43mmmap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmmap\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtagname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    937\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _winapi\u001b[38;5;241m.\u001b[39mGetLastError() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    938\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[1;31mOSError\u001b[0m: [WinError 1455] The paging file is too small for this operation to complete"
     ]
    }
   ],
   "source": [
    "def check_gpu_utilization():\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"\\nCurrent GPU utilization:\")\n",
    "        print(f\"Memory Allocated: {torch.cuda.memory_allocated(0) / 1024**2:.2f} MB\")\n",
    "        print(f\"Memory Cached: {torch.cuda.memory_reserved(0) / 1024**2:.2f} MB\")\n",
    "\n",
    "save_dir = Path(\"checkpoints\") / datetime.datetime.now().strftime(\"%Y-%m-%dT%H-%M-%S\")\n",
    "save_dir.mkdir(parents=True)\n",
    "\n",
    "mario = Mario(state_dim=(4, 84, 84), action_dim=env.action_space.n, save_dir=save_dir)\n",
    "\n",
    "logger = MetricLogger(save_dir)\n",
    "\n",
    "episodes = 5000\n",
    "for e in range(episodes):\n",
    "\n",
    "    state = env.reset()\n",
    "    \n",
    "    if e % 10 == 0:\n",
    "        check_gpu_utilization()\n",
    "\n",
    "    # Play the game!\n",
    "    while True:\n",
    "\n",
    "        # Run agent on the state\n",
    "        action = mario.act(state)\n",
    "\n",
    "        # Agent performs action\n",
    "        next_state, reward, done, trunc, info = env.step(action)\n",
    "\n",
    "        # Remember\n",
    "        mario.cache(state, next_state, action, reward, done)\n",
    "\n",
    "        # Learn\n",
    "        q, loss = mario.learn()\n",
    "\n",
    "        # Logging\n",
    "        logger.log_step(reward, loss, q)\n",
    "\n",
    "        # Update state\n",
    "        state = next_state\n",
    "\n",
    "        # Check if end of game\n",
    "        if done or info[\"flag_get\"]:\n",
    "            break\n",
    "\n",
    "    logger.log_episode()\n",
    "\n",
    "    if (e % 20 == 0) or (e == episodes - 1):\n",
    "        logger.record(episode=e, epsilon=mario.exploration_rate, step=mario.curr_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to trained_mario.chkpt\n"
     ]
    }
   ],
   "source": [
    "save_path = Path('trained_mario.chkpt')\n",
    "torch.save(\n",
    "    dict(\n",
    "        model=mario.net.state_dict(),\n",
    "        exploration_rate=mario.exploration_rate\n",
    "    ),\n",
    "    save_path\n",
    ")\n",
    "print(f\"Model saved to {save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continue training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mario_agent(checkpoint_path, state_dim, action_dim, save_dir, \n",
    "                    new_params=None):\n",
    "    \"\"\"\n",
    "    Load a previously trained Mario agent with option to modify parameters\n",
    "    \n",
    "    Args:\n",
    "        checkpoint_path: Path to the checkpoint file\n",
    "        state_dim: Input state dimensions\n",
    "        action_dim: Number of possible actions\n",
    "        save_dir: Directory to save new checkpoints\n",
    "        new_params: Dictionary of parameters to modify, can include:\n",
    "            - learning_rate: New learning rate\n",
    "            - batch_size: New batch size\n",
    "            - gamma: New discount factor\n",
    "            - exploration_rate: New exploration rate\n",
    "            - exploration_rate_decay: New decay rate\n",
    "            - exploration_rate_min: New minimum exploration rate\n",
    "            - burnin: New burnin period\n",
    "            - learn_every: New learning frequency\n",
    "            - sync_every: New target network sync frequency\n",
    "    \"\"\"\n",
    "    # Initialize new agent\n",
    "    mario = Mario(state_dim=state_dim, \n",
    "                 action_dim=action_dim,\n",
    "                 save_dir=save_dir)\n",
    "    \n",
    "    # Load checkpoint\n",
    "    checkpoint = torch.load(checkpoint_path)\n",
    "    \n",
    "    # Load model state\n",
    "    mario.net.load_state_dict(checkpoint['model'])\n",
    "    mario.exploration_rate = checkpoint['exploration_rate']\n",
    "    \n",
    "    # Modify parameters if specified\n",
    "    if new_params:\n",
    "        if 'learning_rate' in new_params:\n",
    "            mario.optimizer = torch.optim.Adam(mario.net.parameters(), \n",
    "                                             lr=new_params['learning_rate'])\n",
    "        \n",
    "        if 'batch_size' in new_params:\n",
    "            mario.batch_size = new_params['batch_size']\n",
    "            \n",
    "        if 'gamma' in new_params:\n",
    "            mario.gamma = new_params['gamma']\n",
    "            \n",
    "        if 'exploration_rate' in new_params:\n",
    "            mario.exploration_rate = new_params['exploration_rate']\n",
    "            \n",
    "        if 'exploration_rate_decay' in new_params:\n",
    "            mario.exploration_rate_decay = new_params['exploration_rate_decay']\n",
    "            \n",
    "        if 'exploration_rate_min' in new_params:\n",
    "            mario.exploration_rate_min = new_params['exploration_rate_min']\n",
    "            \n",
    "        if 'burnin' in new_params:\n",
    "            mario.burnin = new_params['burnin']\n",
    "            \n",
    "        if 'learn_every' in new_params:\n",
    "            mario.learn_every = new_params['learn_every']\n",
    "            \n",
    "        if 'sync_every' in new_params:\n",
    "            mario.sync_every = new_params['sync_every']\n",
    "    \n",
    "    return mario\n",
    "\n",
    "def continue_training(mario, env, episodes, logger=None):\n",
    "    \"\"\"Continue training the agent for more episodes\"\"\"\n",
    "    \n",
    "    if logger is None:\n",
    "        logger = MetricLogger(mario.save_dir)\n",
    "    \n",
    "    for e in range(episodes):\n",
    "        state = env.reset()\n",
    "\n",
    "        # Play the game!\n",
    "        while True:\n",
    "            # Run agent on the state\n",
    "            action = mario.act(state)\n",
    "\n",
    "            # Agent performs action\n",
    "            next_state, reward, done, trunc, info = env.step(action)\n",
    "\n",
    "            # Remember\n",
    "            mario.cache(state, next_state, action, reward, done)\n",
    "\n",
    "            # Learn\n",
    "            q, loss = mario.learn()\n",
    "\n",
    "            # Logging\n",
    "            logger.log_step(reward, loss, q)\n",
    "\n",
    "            # Update state\n",
    "            state = next_state\n",
    "\n",
    "            # Check if end of game\n",
    "            if done or info[\"flag_get\"]:\n",
    "                break\n",
    "\n",
    "        logger.log_episode()\n",
    "\n",
    "        if e % 20 == 0:\n",
    "            logger.record(\n",
    "                episode=e,\n",
    "                epsilon=mario.exploration_rate,\n",
    "                step=mario.curr_step\n",
    "            )\n",
    "            \n",
    "    return mario, logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Setup directories\n",
    "    save_dir = Path(\"checkpoints\") / datetime.datetime.now().strftime(\"%Y-%m-%dT%H-%M-%S\")\n",
    "    save_dir.mkdir(parents=True)\n",
    "    \n",
    "    # Initialize environment\n",
    "    env = gym_super_mario_bros.make(\"SuperMarioBros-1-1-v0\", apply_api_compatibility=True)\n",
    "    env = JoypadSpace(env, MOVEMENT_OPTIONS)\n",
    "    env = SkipFrame(env, skip=4)\n",
    "    env = GrayScaleObservation(env)\n",
    "    env = ResizeObservation(env, shape=84)\n",
    "    env = FrameStack(env, num_stack=4)\n",
    "    \n",
    "    # Example parameters to modify\n",
    "    new_params = {\n",
    "        'learning_rate': 0.00025,\n",
    "        'exploration_rate': 0.5,\n",
    "        'exploration_rate_decay': 0.99999,\n",
    "        'batch_size': 512\n",
    "    }\n",
    "    \n",
    "    # Load and modify agent\n",
    "    mario = load_mario_agent(\n",
    "        checkpoint_path='trained_mario.chkpt',\n",
    "        state_dim=(4, 84, 84),\n",
    "        action_dim=env.action_space.n,\n",
    "        save_dir=save_dir,\n",
    "        new_params=new_params\n",
    "    )\n",
    "    \n",
    "    # Continue training\n",
    "    mario, logger = continue_training(\n",
    "        mario=mario,\n",
    "        env=env,\n",
    "        episodes=1000\n",
    "    )\n",
    "    \n",
    "    # Save the newly trained model\n",
    "    current_datetime = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    save_path = Path(f'models/trained_mario_{current_datetime}.chkpt')\n",
    "    torch.save(\n",
    "        dict(\n",
    "            model=mario.net.state_dict(),\n",
    "            exploration_rate=mario.exploration_rate\n",
    "        ),\n",
    "        save_path\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Setup directories\n",
    "    save_dir = Path(\"checkpoints\") / datetime.datetime.now().strftime(\"%Y-%m-%dT%H-%M-%S\")\n",
    "    save_dir.mkdir(parents=True)\n",
    "    \n",
    "    # Initialize environment\n",
    "    env = gym_super_mario_bros.make(\"SuperMarioBros-1-1-v0\", apply_api_compatibility=True)\n",
    "    env = JoypadSpace(env, MOVEMENT_OPTIONS)\n",
    "    env = SkipFrame(env, skip=4)\n",
    "    env = GrayScaleObservation(env)\n",
    "    env = ResizeObservation(env, shape=84)\n",
    "    env = FrameStack(env, num_stack=4)\n",
    "    \n",
    "    # Example parameters to modify\n",
    "    new_params = {\n",
    "        'learning_rate': 0.00025,\n",
    "        'exploration_rate': 0.5,\n",
    "        'exploration_rate_decay': 0.99999,\n",
    "        'batch_size': 512\n",
    "    }\n",
    "    \n",
    "    # Load and modify agent\n",
    "    mario = load_mario_agent(\n",
    "        checkpoint_path='trained_mario.chkpt',\n",
    "        state_dim=(4, 84, 84),\n",
    "        action_dim=env.action_space.n,\n",
    "        save_dir=save_dir,\n",
    "        new_params=new_params\n",
    "    )\n",
    "    \n",
    "    # Continue training\n",
    "    mario, logger = continue_training(\n",
    "        mario=mario,\n",
    "        env=env,\n",
    "        episodes=1000\n",
    "    )\n",
    "    \n",
    "    # Save the newly trained model\n",
    "    current_datetime = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    save_path = Path(f'models/cont_trained_mario_{current_datetime}.chkpt')\n",
    "    torch.save(\n",
    "        dict(\n",
    "            model=mario.net.state_dict(),\n",
    "            exploration_rate=mario.exploration_rate\n",
    "        ),\n",
    "        save_path\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
